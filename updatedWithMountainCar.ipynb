{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ddb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run once)\n",
    "!pip install -q torch gymnasium numpy matplotlib wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & device setup\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# If using Weights & Biases, uncomment after logging in interactively:\n",
    "# wandb.login()\n",
    "wandb.init(project=\"RL_Assignment2_DQN_DDQN\", name=\"DQN_DDQN_Final\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((np.array(state, dtype=np.float32),\n",
    "                            int(action),\n",
    "                            float(reward),\n",
    "                            np.array(next_state, dtype=np.float32),\n",
    "                            bool(done)))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network Definition\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = nn.Linear(hidden_units, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze = True\n",
    "        else:\n",
    "            squeeze = False\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        if squeeze:\n",
    "            return out.squeeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c79322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN and DDQN Agents\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, config):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = config.get(\"gamma\", 0.99)\n",
    "        self.epsilon = config.get(\"epsilon_start\", 1.0)\n",
    "        self.epsilon_min = config.get(\"epsilon_min\", 0.01)\n",
    "        self.epsilon_decay = config.get(\"epsilon_decay\", 0.999)\n",
    "        self.lr = config.get(\"lr\", 1e-3)\n",
    "        self.batch_size = config.get(\"batch_size\", 64)\n",
    "        self.grad_clip = config.get(\"grad_clip\", 10.0)\n",
    "\n",
    "        self.q_net = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "        self.memory = ReplayBuffer(config.get(\"memory_size\", 100000))\n",
    "        self.loss_fn = F.smooth_l1_loss\n",
    "\n",
    "    def act_index(self, state, deterministic=False):\n",
    "        if (not deterministic) and (random.random() < self.epsilon):\n",
    "            return random.randrange(self.action_dim)\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).to(device)\n",
    "            qvals = self.q_net(s)\n",
    "            return int(torch.argmax(qvals).item())\n",
    "\n",
    "    def remember(self, s, a, r, ns, done):\n",
    "        self.memory.push(s, a, r, ns, done)\n",
    "\n",
    "    def replay_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones.astype(np.float32)).unsqueeze(1).to(device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_vals = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            targets = rewards + self.gamma * next_q_vals * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        return float(loss.item())\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "        torch.save(self.q_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path, map_location=None):\n",
    "        self.q_net.load_state_dict(torch.load(path, map_location=map_location or device))\n",
    "        self.update_target()\n",
    "\n",
    "class DDQNAgent(DQNAgent):\n",
    "    def replay_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones.astype(np.float32)).unsqueeze(1).to(device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_net(next_states).argmax(1).unsqueeze(1)\n",
    "            next_q_vals = self.target_net(next_states).gather(1, next_actions)\n",
    "            targets = rewards + self.gamma * next_q_vals * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        return float(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63994641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Discretization Helper\n",
    "def make_action_list(env, resolution=7, env_name=None):\n",
    "    \"\"\"Return (action_size, actions_list) for Discrete or Box spaces.\n",
    "    Discrete -> integer list; Box(1,) -> scalar list; multi-dim Box -> vary first dim only.\n",
    "    Pendulum gets higher default resolution.\n",
    "    \"\"\"\n",
    "    act_space = env.action_space\n",
    "    if isinstance(act_space, gym.spaces.Discrete):\n",
    "        n = act_space.n\n",
    "        return n, list(range(n))\n",
    "    elif isinstance(act_space, gym.spaces.Box):\n",
    "        low = act_space.low\n",
    "        high = act_space.high\n",
    "        shape = act_space.shape\n",
    "        res = max(resolution, 15) if env_name == \"Pendulum-v1\" else resolution\n",
    "        if len(shape) == 1 and shape[0] == 1:\n",
    "            vals = np.linspace(low[0], high[0], res)\n",
    "            return len(vals), [float(v) for v in vals]\n",
    "        else:\n",
    "            vals = np.linspace(low[0], high[0], res)\n",
    "            actions = [np.array([v] + [0]*(shape[0]-1), dtype=act_space.dtype) for v in vals]\n",
    "            return len(actions), actions\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function (warmup, MountainCar shaping, action formatting)\n",
    "def train_agent(env_name, agent_type=\"DQN\", episodes=150, seed=0, save_model=True,\n",
    "                warmup_steps=2000, target_update_steps=1000, discretize_res=7):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    env = gym.make(env_name)\n",
    "    env_details = {\"obs_space\": str(env.observation_space), \"act_space\": str(env.action_space)}\n",
    "\n",
    "    if env_name == \"MountainCar-v0\" and episodes < 1000:\n",
    "        print(\"Note: overriding episodes -> 1000 for MountainCar\")\n",
    "        episodes = 1000\n",
    "        warmup_steps = max(warmup_steps, 3000)\n",
    "        target_update_steps = 500\n",
    "\n",
    "    action_size, actions_list = make_action_list(env, resolution=discretize_res, env_name=env_name)\n",
    "    obs_shape = env.observation_space.shape\n",
    "    state_size = int(np.prod(obs_shape)) if obs_shape else 1\n",
    "\n",
    "    config = {\n",
    "        \"gamma\": 0.99, \"epsilon_start\": 1.0, \"epsilon_min\": 0.01,\n",
    "        \"epsilon_decay\": 0.999, \"lr\": 1e-3, \"memory_size\": 100000,\n",
    "        \"batch_size\": 64, \"grad_clip\": 10.0\n",
    "    }\n",
    "    if env_name == \"MountainCar-v0\":\n",
    "        config[\"epsilon_decay\"] = 0.9995; config[\"lr\"] = 5e-4\n",
    "    if env_name == \"CartPole-v1\":\n",
    "        config[\"epsilon_decay\"] = 0.995\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        config[\"epsilon_decay\"] = 0.999\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size, config) if agent_type == \"DQN\" else DDQNAgent(state_size, action_size, config)\n",
    "\n",
    "    models_dir = os.path.join(\"models\", env_name); os.makedirs(models_dir, exist_ok=True)\n",
    "    normalize = (env_name == \"MountainCar-v0\")\n",
    "    obs_low, obs_high = (env.observation_space.low, env.observation_space.high) if normalize else (None, None)\n",
    "\n",
    "    print(f\"[Warmup] populating replay buffer with {warmup_steps} random steps...\")\n",
    "    state_raw, _ = env.reset(seed=seed)\n",
    "    state = (state_raw - obs_low) / (obs_high - obs_low) if normalize else state_raw\n",
    "    for _ in range(warmup_steps):\n",
    "        idx = random.randrange(action_size); action = actions_list[idx]\n",
    "        if isinstance(env.action_space, gym.spaces.Box):\n",
    "            action_to_env = np.array([action], dtype=env.action_space.dtype) if np.isscalar(action) else np.array(action, dtype=env.action_space.dtype)\n",
    "        else:\n",
    "            action_to_env = int(action)\n",
    "        next_state_raw, r_env, terminated, truncated, _ = env.step(action_to_env)\n",
    "        done = terminated or truncated\n",
    "        next_state = (next_state_raw - obs_low) / (obs_high - obs_low) if normalize else next_state_raw\n",
    "        agent.remember(state, idx, r_env, next_state, done)\n",
    "        state_raw = next_state_raw; state = next_state\n",
    "        if done:\n",
    "            state_raw, _ = env.reset(); state = (state_raw - obs_low) / (obs_high - obs_low) if normalize else state_raw\n",
    "\n",
    "    rewards_list = []; best_reward = -float(\"inf\"); step_count = 0\n",
    "    alpha = 10.0; beta = 0.05; goal_reward = 100.0\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "        state_raw, _ = env.reset(seed=seed + ep)\n",
    "        state = (state_raw - obs_low) / (obs_high - obs_low) if normalize else state_raw\n",
    "        total_reward = 0.0; done = False\n",
    "        while not done:\n",
    "            idx = agent.act_index(state); action = actions_list[idx]\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                action_to_env = np.array([action], dtype=env.action_space.dtype) if np.isscalar(action) else np.array(action, dtype=env.action_space.dtype)\n",
    "            else:\n",
    "                action_to_env = int(action)\n",
    "            next_state_raw, r_env, terminated, truncated, _ = env.step(action_to_env)\n",
    "            done = terminated or truncated\n",
    "            if env_name == \"MountainCar-v0\":\n",
    "                pos_min, pos_max = env.observation_space.low[0], env.observation_space.high[0]\n",
    "                pos_cur, vel_cur = float(state_raw[0]), float(state_raw[1])\n",
    "                pos_next, vel_next = float(next_state_raw[0]), float(next_state_raw[1])\n",
    "                pos_cur_norm = (pos_cur - pos_min) / (pos_max - pos_min)\n",
    "                pos_next_norm = (pos_next - pos_min) / (pos_max - pos_min)\n",
    "                phi_cur = alpha * pos_cur_norm; phi_next = alpha * pos_next_norm\n",
    "                shaping = agent.gamma * phi_next - phi_cur\n",
    "                vel_bonus = beta * abs(vel_next)\n",
    "                reward = float(r_env + shaping + vel_bonus)\n",
    "                if done and pos_next >= 0.5: reward += goal_reward\n",
    "            else:\n",
    "                reward = float(r_env)\n",
    "            next_state = (next_state_raw - obs_low) / (obs_high - obs_low) if normalize else next_state_raw\n",
    "            agent.remember(state, idx, reward, next_state, done); _ = agent.replay_step()\n",
    "            state_raw = next_state_raw; state = next_state\n",
    "            total_reward += reward; step_count += 1\n",
    "            if step_count % target_update_steps == 0: agent.update_target()\n",
    "        rewards_list.append(total_reward)\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            ckpt_path = os.path.join(models_dir, f\"{env_name}_{agent_type}_ep{ep}_rew{int(total_reward)}.pth\")\n",
    "            agent.save(ckpt_path); agent.save(os.path.join(models_dir, f\"{env_name}_{agent_type}_best.pth\"))\n",
    "        try:\n",
    "            wandb.log({f\"{agent_type}/{env_name}/reward\": float(total_reward), f\"{agent_type}/{env_name}/epsilon\": float(agent.epsilon)})\n",
    "        except Exception:\n",
    "            pass\n",
    "        if (ep % 10 == 0) or (ep == 1) or (ep == episodes):\n",
    "            print(f\"{agent_type} | {env_name} | Episode {ep}/{episodes} | Reward: {total_reward:.2f} | Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    final_path = os.path.join(models_dir, f\"{env_name}_{agent_type}_final.pth\"); agent.save(final_path); env.close()\n",
    "    meta = {\"env_details\": env_details, \"actions_list\": actions_list, \"model_path\": final_path}\n",
    "    return agent, rewards_list, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ea9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation + Video Recording\n",
    "def evaluate_and_record(agent, env_name, actions_list, agent_type=\"DQN\", episodes=3, seed=42):\n",
    "    video_folder = f\"videos/{env_name}/{agent_type}\"; os.makedirs(video_folder, exist_ok=True)\n",
    "    try:\n",
    "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    except TypeError:\n",
    "        base_env = gym.make(env_name)\n",
    "    env = RecordVideo(base_env, video_folder=video_folder, episode_trigger=lambda e: True)\n",
    "\n",
    "    details = {\"observation_space\": str(base_env.observation_space), \"action_space\": str(base_env.action_space)}\n",
    "    try:\n",
    "        wandb.log({f\"{agent_type}/{env_name}/env_description\": str(details)})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    all_episode_rewards = []\n",
    "    normalize = (env_name == \"MountainCar-v0\")\n",
    "    if normalize: obs_low, obs_high = base_env.observation_space.low, base_env.observation_space.high\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state_raw, _ = env.reset(seed=seed + ep)\n",
    "        state = (state_raw - obs_low) / (obs_high - obs_low) if normalize else state_raw\n",
    "        done = False; total_reward = 0.0; steps = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                action_idx = agent.act_index(state, deterministic=True)\n",
    "            except TypeError:\n",
    "                action_idx = agent.act_index(state)\n",
    "            action = actions_list[action_idx]\n",
    "            if isinstance(base_env.action_space, gym.spaces.Box):\n",
    "                action_to_env = np.array([action], dtype=base_env.action_space.dtype) if np.isscalar(action) else np.array(action, dtype=base_env.action_space.dtype)\n",
    "            else:\n",
    "                action_to_env = int(action)\n",
    "            next_state_raw, r, terminated, truncated, _ = env.step(action_to_env)\n",
    "            done = terminated or truncated\n",
    "            next_state = (next_state_raw - obs_low) / (obs_high - obs_low) if normalize else next_state_raw\n",
    "            state = next_state; total_reward += float(r); steps += 1\n",
    "        all_episode_rewards.append(total_reward)\n",
    "        print(f\"[VIDEO] {agent_type} | {env_name} | Eval {ep+1}/{episodes} | Reward: {total_reward:.2f} | Steps: {steps}\")\n",
    "\n",
    "    env.close()\n",
    "    vids = sorted([os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith(\".mp4\")])\n",
    "    print(f\"Saved {len(vids)} videos to {video_folder}\")\n",
    "    for i, v in enumerate(vids):\n",
    "        try:\n",
    "            wandb.log({f\"{agent_type}/{env_name}/video_{i}\": wandb.Video(v, fps=30, format=\"mp4\")})\n",
    "        except Exception:\n",
    "            print(\"W&B video upload failed or unavailable.\")\n",
    "    return all_episode_rewards, vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ada6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Stability Tests (100 episodes)\n",
    "def evaluate_agent_tests(agent, env_name, actions_list, n_tests=100, deterministic=True):\n",
    "    env = gym.make(env_name)\n",
    "    normalize = (env_name == \"MountainCar-v0\")\n",
    "    if normalize: obs_low, obs_high = env.observation_space.low, env.observation_space.high\n",
    "    durations, rewards = [], []\n",
    "    for t in range(n_tests):\n",
    "        state_raw, _ = env.reset(seed=10000 + t)\n",
    "        state = (state_raw - obs_low) / (obs_high - obs_low) if normalize else state_raw\n",
    "        done = False; total_reward = 0.0; steps = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                idx = agent.act_index(state, deterministic=deterministic)\n",
    "            except TypeError:\n",
    "                idx = agent.act_index(state)\n",
    "            action = actions_list[idx]\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                action_to_env = np.array([action], dtype=env.action_space.dtype) if np.isscalar(action) else np.array(action, dtype=env.action_space.dtype)\n",
    "            else:\n",
    "                action_to_env = int(action)\n",
    "            next_state_raw, r, terminated, truncated, _ = env.step(action_to_env)\n",
    "            done = terminated or truncated\n",
    "            next_state = (next_state_raw - obs_low) / (obs_high - obs_low) if normalize else next_state_raw\n",
    "            state = next_state; total_reward += float(r); steps += 1\n",
    "        durations.append(steps); rewards.append(total_reward)\n",
    "    env.close(); return np.array(durations), np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd476f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Helper (Training Curves)\n",
    "def plot_rewards(env_name, results_dict):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Training Rewards: {env_name}\")\n",
    "    if len(results_dict[\"DQN\"][\"rewards\"])>0:\n",
    "        plt.plot(results_dict[\"DQN\"][\"rewards\"], label=\"DQN\")\n",
    "    if len(results_dict[\"DDQN\"][\"rewards\"])>0:\n",
    "        plt.plot(results_dict[\"DDQN\"][\"rewards\"], label=\"DDQN\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"Reward\"); plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14095871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration: Train Across Environments\n",
    "envs = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "results = {}\n",
    "for env_name in envs:\n",
    "    episodes = 1000 if env_name == \"MountainCar-v0\" else 150\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TRAINING {env_name} with DQN for {episodes} episodes\")\n",
    "    dqn_agent, dqn_rewards, dqn_meta = train_agent(env_name, agent_type=\"DQN\", episodes=episodes, seed=0)\n",
    "    print(f\"TRAINING {env_name} with DDQN for {episodes} episodes\")\n",
    "    ddqn_agent, ddqn_rewards, ddqn_meta = train_agent(env_name, agent_type=\"DDQN\", episodes=episodes, seed=0)\n",
    "    results[env_name] = {\n",
    "        \"DQN\": {\"agent\": dqn_agent, \"rewards\": dqn_rewards, \"meta\": dqn_meta},\n",
    "        \"DDQN\": {\"agent\": ddqn_agent, \"rewards\": ddqn_rewards, \"meta\": ddqn_meta},\n",
    "    }\n",
    "    print(f\"\\nRecording evaluation videos for {env_name} with DQN\")\n",
    "    dqn_eval_rewards, dqn_videos = evaluate_and_record(dqn_agent, env_name, dqn_meta[\"actions_list\"], agent_type=\"DQN\", episodes=3)\n",
    "    print(f\"\\nRecording evaluation videos for {env_name} with DDQN\")\n",
    "    ddqn_eval_rewards, ddqn_videos = evaluate_and_record(ddqn_agent, env_name, ddqn_meta[\"actions_list\"], agent_type=\"DDQN\", episodes=3)\n",
    "    results[env_name][\"DQN\"].update({\"eval_rewards\": dqn_eval_rewards, \"videos\": dqn_videos})\n",
    "    results[env_name][\"DDQN\"].update({\"eval_rewards\": ddqn_eval_rewards, \"videos\": ddqn_videos})\n",
    "    print(f\"\\nRunning stability tests (100 deterministic) for {env_name} - DQN\")\n",
    "    d_durations, d_rewards = evaluate_agent_tests(dqn_agent, env_name, dqn_meta[\"actions_list\"], n_tests=100, deterministic=True)\n",
    "    print(f\"DQN {env_name} durations mean/std: {d_durations.mean():.2f}/{d_durations.std():.2f} | rewards mean/std: {d_rewards.mean():.2f}/{d_rewards.std():.2f}\")\n",
    "    print(f\"Running stability tests (100 deterministic) for {env_name} - DDQN\")\n",
    "    dd_durations, dd_rewards = evaluate_agent_tests(ddqn_agent, env_name, ddqn_meta[\"actions_list\"], n_tests=100, deterministic=True)\n",
    "    print(f\"DDQN {env_name} durations mean/std: {dd_durations.mean():.2f}/{dd_durations.std():.2f} | rewards mean/std: {dd_rewards.mean():.2f}/{dd_rewards.std():.2f}\")\n",
    "    try:\n",
    "        wandb.log({\n",
    "            f\"DQN/{env_name}/test/dur_mean\": float(d_durations.mean()),\n",
    "            f\"DQN/{env_name}/test/dur_std\": float(d_durations.std()),\n",
    "            f\"DQN/{env_name}/test/rew_mean\": float(d_rewards.mean()),\n",
    "            f\"DQN/{env_name}/test/rew_std\": float(d_rewards.std()),\n",
    "            f\"DDQN/{env_name}/test/dur_mean\": float(dd_durations.mean()),\n",
    "            f\"DDQN/{env_name}/test/dur_std\": float(dd_durations.std()),\n",
    "            f\"DDQN/{env_name}/test/rew_mean\": float(dd_rewards.mean()),\n",
    "            f\"DDQN/{env_name}/test/rew_std\": float(dd_rewards.std()),\n",
    "        })\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae278de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
